---
output:
  #bookdown::html_document2: default
  #bookdown::word_document2: default
  bookdown::pdf_document2: 
    template: templates/brief_template.tex
    citation_package: biblatex
documentclass: book
bibliography: references.bib
---

# Data and methodology {#dat-and-meth}

\chaptermark{Data and methodology}

\minitoc <!-- this will include a mini table of contents-->

## Data

<!-- uncomment this codes -->

Here comes text...

```{r dataloading, include=F}
require(readxl)
require(xts)
require(PerformanceAnalytics)
require(kableExtra)
require(rugarch)
require(fitdistrplus)
require(fGarch)
require(sn)
require(tree)


options(warn = -1, scipen = 999)
data <- read_excel("data/datastream.xlsx",col_types = c("date", rep("numeric", 6)),
                                       skip = 2)
colnames(data) <- c("Date",gsub(pattern = " ", replacement = "_",gsub(pattern = " - PRICE INDEX", replacement='' , colnames(data)[2:7])))
Price_indices <- as.xts(data[,-1], order.by = data$Date)
Estoxx <- Price_indices[,1] #see if price index
R <- diff(Estoxx, log = TRUE, na.pad = FALSE)*100

```

### Descriptives

#### Table of summary statistics

Here comes a table and description of the stats

```{r stats, echo=F, results='asis'}
Statistics <-  table.Stats(R)

Stats.names <- rownames(Statistics)[-c(1,2,4,7,8,10:13)]
Statistics <- Statistics[-c(1,2,4,7,8,10:13),] #selecting relevant columns only
names(Statistics) <- Stats.names


garchspec.R <- ugarchspec(mean.model = list(armaOrder = c(1,0)),
                     variance.model = list(model = "sGARCH", variance.targeting = F), 
                     distribution.model = "norm")
# Estimate the model
garchfit.R <- ugarchfit(data = R, spec = garchspec.R)

# Compute stdret using residuals()
stdret.R <- residuals(garchfit.R, standardize = TRUE)

Statistics.S <-  table.Stats(stdret.R)
Statistics.S <- Statistics.S[-c(1,2,4,7,8,10:13),] #selecting relevant columns only
# Stats.S.names <- rownames(Statistics.S)[-c(1,2,4,7,8,10:13)]



kable(cbind(Statistics, Statistics.S), caption = "Summary statistics of the returns",
      label = 'dsTable',
      booktabs = T,
      position = "h!",
      digits = 3 )%>%
  kable_classic(full_width = F)%>%
  footnote('This table shows the descriptive statistics of the returns of the 5 asset classes over the period %s to %s',
           footnote_as_chunk = T, general_title = "Note: ")

```

#### Correlation

Here comes a table and description of the correlations

```{r correlation, echo=F}
# mcor <- cor(R)
# upper <- round(mcor,3)
# library(xtable)
# upper[upper.tri(mcor)] <-""
# upper<-as.data.frame(upper)
# upper %>% kbl(format='latex',
#       caption = "Correlation table between the different blue chip indices",
#       label = 'dsTable',
#       booktabs = T,
#       position = "h!",
#       digits = 3) %>%
#   kable_classic(full_width = F) %>%
#   footnote(general = sprintf('This table shows the descriptive statistics of the returns of the %s asset classes over the period %s to %s', ncol(data), data$...1[2],data$...1[nrow(data)]),
#            footnote_as_chunk = T,
#            general_title = 'Note: ',
#            threeparttable = TRUE)
```

#### Visualizations (eye-balling)

```{r plot1, echo=F, fig.cap='Eurostoxx 50 rolling volatility'}
#par(mfrow = c(1,1))
plot(Estoxx, major.ticks = "years", grid.ticks.on = "years", col = "blue",
     main = "EuroStoxx 50 Price Index")
plot(R, major.ticks = "years", grid.ticks.on = "years", col = "blue",
     main = "EuroStoxx 50 Price Log Returns")
```

```{r plot2, echo=F}
#Histogram to eyeball normality

# h <- hist(R, breaks = 75, density = 10,
#           col = "lightgray", xlab = "Accuracy", main = "Barplot") 
# xfit <- seq(min(R), max(R), length = 40) 
# yfit <- dnorm(xfit, mean = mean(R), sd = sd(R)) 
# yfit <- yfit * diff(h$mids[1:2]) * length(R) 
# lines(xfit, yfit, col = "black", lwd = 1)

chart.Histogram(R = R, methods = "add.normal", breaks = 100, main = "Returns Histogram Vs. Normal")

#Plotting volatility

chart.RollingPerformance(R = R, width = 22,
     FUN = "sd.annualized", scale = 252, main = "One month rolling volatility")

```

As can be seen

```{r acfplots, echo=F}

# Compute the mean daily return
m <- mean(R)

# Define the series of prediction errors
e <- R - m

# Plot the absolute value of the prediction errors
par(mfrow = c(2,1),mar = c(1, 2, 2, 1))
plot(abs(e), main = "Absolute Prediction Error")

# Plot the acf of the absolute prediction errors
acf(abs(e), main = "", xaxt = "n")

# acf(coredata(R), lag = 22, main = "EuroStoxx 50 Price Log Returns")
# acf(coredata(abs(R)), lag = 22, main = "EuroStoxx 50 Absolute Price Log Returns")
```

```{r}
distributions <- c("norm", "std", "sstd", "sged", "ged")
garchspec <- garchfit <- garchforecast <- stdret <- vector(mode = "list", length = length(distributions))

for(i in 1:length(distributions)){
# Specify a GARCH model with constant mean
garchspec[[i]] <- ugarchspec(mean.model = list(armaOrder = c(1,0)),
                     variance.model = list(model = "sGARCH", variance.targeting = F), 
                     distribution.model = distributions[i])
# Estimate the model
garchfit[[i]] <- ugarchfit(data = R, spec = garchspec[[i]])

# Compute stdret using residuals()
stdret[[i]] <- residuals(garchfit[[i]], standardize = TRUE)

# Compute stdret using fitted() and sigma()
stdret[[i]] <- (R - fitted(garchfit[[i]])) / sigma(garchfit[[i]]) 

}

#  make the histogram

chart.Histogram(stdret[[1]], methods = c("add.normal","add.density" ), 
                colorset = c("gray","red","blue"))


```

\newpage

### Methodology

As already mentioned in ..., GARCH models sGARCH, eGARCH, iGARCH, gjrGARCH, nGARCH, tGARCH and tsGARCH will be estimated. Additionally the distributions will be examined as well, including the normal, student-t distribution, skewed student-t distribution, generalised error distribution, skewed generalised error distribution and the skewed generalised Theodossiou distribution.

<<<<<<< Updated upstream
##
They will be estimated using maximum likelihood. As already mentioned, fortunately, Alexios @alexios2020 has made it easy for us to implement this methodology in R (version 3.6.1) with the package rugarch version 1.4-4 (R univariate garch), which gives us a bit more time to focus on the results and the interpretation.

Maximum likelihood estimation is a method to find the distribution parameters that best fit the observed data, through maximization of the likelihood function, or the computationally more efficient log-likelihood function (by taking the natural logarithm). It is assumed that the return data is i.i.d. and that there is some underlying parametrized density function $f$ with one or more parameters $\theta$ that generates the data (\@ref(eq:eq1)). These functions are based on the joint probability distribution of the observed data (equation \@ref(eq:eq2)). Subsequently, the (log)likelihood function is maximized using an optimization algorithm (equation \@ref(eq:eq3)).

```{=tex}
\begin{align} 
     y_1,y_2,...,y_N \sim i.i.d
    \\
    y_i \sim f(y|\theta)
 (\#eq:eq1)
\end{align}
```
```{=tex}
\begin{align} 

 L(\theta) = \prod^{N}_{i=1}f(y_i|\theta)\\
\log(L(\theta)) = \sum^{N}_{i=1} \log f(y_i |\theta)

 (\#eq:eq2)
\end{align}
```
```{=tex}
\begin{align} 

\theta^{*} = arg \max_{\theta} \bigg[ {(L)} \bigg]
\\theta^{*} = arg \max_{\theta} \bigg[ \log{(L)} \bigg]

 (\#eq:eq3)
\end{align}
```

```{r MLE exampe}




n_samples <- 25; true_rate <- 1; set.seed(1)
exp_samples <- rexp(n = n_samples,
                    rate = true_rate)

sample_data <- exp_samples
rate_fit_R <- fitdistrplus::fitdist(data = sample_data, 
                      distr = 'exp', 
                      method = 'mle')

rate_fit_R$estimate
```

```{r MLE normdist}
normfit_R <- fitdistrplus::fitdist(data = as.vector(R), 
                      distr = "norm",
                      method = "mle")

normfit_R$estimate #estimated parameters
normfit_R$sd #estimated standard errors
normfit_R$loglik #max log-likelihood 
#? # LR statistic



tstart <- list(df=1, ncp = 1)

tfit_R <- fitdistrplus::fitdist(data = as.vector(R), 
                      distr = "t",
                      method = "mle", start = tstart)

tfit_R$estimate
tfit_R$sd
normfit_R$loglik
#? # LR statistic


tfit_R$estimate #estimated parameters
tfit_R$sd #estimated standard errors
tfit_R$loglik #max log-likelihood 
#? # LR statistic

gedstart <- list(mean=1, sd=1, nu=1)
gedfit_R <- fitdistrplus::fitdist(data = as.vector(R), 
                      distr = "ged",
                      method = "mle", start = gedstart)
gedfit_R$estimate
gedfit_R$sd
gedfit_R$loglik
#? # LR statistic

skewtstart <- list(xi=1, omega=1, alpha=1, nu=1)
skewtfit_R <- fitdistrplus::fitdist(data = as.vector(R), 
                      distr = "st",
                      method = "mle", skewtstart)

skewtfit_R$estimate
skewtfit_R$sd
skewtfit_R$loglik
#? # LR statistic



#example for professors
#data example 

x <- c(1,2,-3,1,1,1,-1,-1,1,0,1)
#t distribution
tstart <- list(df=1, ncp = 1)

tfit_R <- fitdistrplus::fitdist(data = as.vector(R), 
                      distr = "t",
                      method = "mle", start = tstart)

#SGT distribution


SGTstart <- list(mu=0.,sigma=1, lambda = 0.5, p=2, q=Inf)
SGTfit_R <- fitdistrplus::fitdist(data = x, 
                      distr = "sgt",
                      method = "mle", SGTstart)

```


=======
They will be estimated using maximum likelihood. As already mentioned, fortunately, Alexios @alexios2020 has made it easy for us to implement this methodology in R (version 3.6.1) with the package rugarch version 1.4-4 (R univariate garch), which gives us a bit more time to focus on the results and the interpretation.
>>>>>>> Stashed changes

#### Maximum likelihood estimation

#### Control Tests

##### Unconditional coverage test of @kupiec1995

A number of tests are computed to see if the value-at-risk estimations capture the actual losses well. A first one is the unconditional coverage test by @kupiec1995. The unconditional coverage or proportion of failures method tests if the actual value-at-risk exceedances are consistent with the expected exceedances (a chosen percentile, e.g. 1% percentile) of the VaR model. Following @kupiec1995 and @ghalanos2020, the number of exceedances follow a binomial distribution under the null hypothesis of a correct VaR model. The test is conducted as a likelihood ratio test with statistic like in equation \@ref(eq:uccov), with $p$ the probability of an exceedance for a confidence level and $N$ the sample size. The null hypothesis states that the test statistic $L R^{u c}$ is $\chi^2$-distributed with one degree of freedom.

```{=tex}
\begin{aligned}
L R^{u c}=-2 \ln \left(\frac{(1-p)^{N-X} p^{X}}{\left(1-\frac{X}{N}\right)^{N-X}\left(\frac{X}{N}\right)^{X}}\right)
(\#eq:uccov)
\end{aligned}
```
##### Conditional coverage test of @christoffersen2001

\clearpage
