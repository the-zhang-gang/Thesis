---
output:
  #bookdown::html_document2: default
  #bookdown::word_document2: default
  bookdown::pdf_document2: 
    template: templates/brief_template.tex
    citation_package: biblatex
documentclass: book
bibliography: references.bib
---

# Data and methodology {#dat-and-meth}

\chaptermark{Data and methodology}

\minitoc <!-- this will include a mini table of contents-->

## Data

<!-- uncomment this codes -->

Here comes text...

```{r dataloading, include=F}
require(readxl)
require(xts)
require(PerformanceAnalytics)
require(kableExtra)
require(rugarch)
require(fitdistrplus)
require(fGarch) 
require(tree)  # do we need this?
require(sgt)
require(tseries)
require(openxlsx)

options(scipen = 999)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
data <- read_excel("data/datastream.xlsx",col_types = c("date", rep("numeric", 6)),skip = 2) #warnings are NA's
colnames(data) <- c("Date",gsub(pattern = " - PRICE INDEX", replacement='' , colnames(data)[2:7]))
Price_indices <- as.xts(data[,-1], order.by = data$Date)
Estoxx <- Price_indices[,1] #see if price index
R <- diff(Estoxx, log = TRUE, na.pad = FALSE)*100

write.xlsx(R, "data/Eurostoxx50.xlsx", sheetName = "Eurostoxx50", 
  col.names = TRUE, row.names = TRUE, append = FALSE)
```

### Descriptives

#### Table of summary statistics

Here comes a table and description of the stats

<!--# [FILIPPO] -> to do: describe it! Also do we have to add a significance level for skewness and kurtosis like in the paper? think they do a t-test or?  -->

```{r stats, echo=F, results='asis'}
## RETURNS

#selecting relevant statistics only from table.Stats
Statistics <-  table.Stats(R)
Stats.names <- rownames(Statistics)[-c(1,2,4,7,8,10:13)]
Statistics <- Statistics[-c(1,2,4,7,8,10:13),] 
names(Statistics) <- Stats.names

# Skewness test
skewtest <- normtest::skewness.norm.test(coredata(R)) #as you can see the skewness is the same as the table.Stats method
Skewness <- round(skewtest$statistic,4)
names(Skewness) <- "Skewness"
skewness.pvalue <- skewtest$p.value
kurttest <- normtest::kurtosis.norm.test(coredata(R)) #this is normal kurtosis (not excess)
Excess_kurtosis <- round(kurttest$statistic,4) - 3
names(Excess_kurtosis) <- "Excess Kurtosis"
kurtosis.pvalue <- kurttest$p.value

# Jb test
robustjb_R <- DescTools::JarqueBeraTest(coredata(R)) #robust?
jb_R <- normtest::jb.norm.test(R)

jb_R <- paste0(round(jb_R$statistic,4),"***")
names(jb_R) <- "Jarque-Bera"

Statistics <-  c(Statistics[1:5], Skewness, paste0("(",skewness.pvalue,"***)"), Excess_kurtosis, paste0("(",kurtosis.pvalue,"***)"),jb_R)

## STANDARDIZED RESIDUALS
garchspec_R <- ugarchspec(mean.model = list(armaOrder = c(1,0)),
                     variance.model = list(model = "sGARCH", variance.targeting = F), 
                     distribution.model = "norm")
# Estimate the model
garchfit_R <- ugarchfit(data = R, spec = garchspec.R)

# Compute stdret using residuals()
stdret_R <- residuals(garchfit.R, standardize = TRUE)

Statistics.S <-  table.Stats(stdret.R)
Stats.names.S <- rownames(Statistics.S)[-c(1,2,4,7,8,10:13)]
Statistics.S <- Statistics.S[-c(1,2,4,7,8,10:13),] #selecting relevant columns only
# Stats.S.names <- rownames(Statistics.S)[-c(1,2,4,7,8,10:13)]

Statistics.S <- Statistics.S[,1]
names(Statistics.S) <- Stats.names.S

# Skewness test
skewtest <- normtest::skewness.norm.test(coredata(stdret.R)) #as you can see the skewness is the same as the table.Stats method
Skewness <- round(skewtest$statistic,4)
names(Skewness) <- "Skewness"
skewness.pvalue <- skewtest$p.value
kurttest <- normtest::kurtosis.norm.test(coredata(stdret.R)) #this is normal kurtosis (not excess)
Excess_kurtosis <- round(kurttest$statistic,4) - 3
names(Excess_kurtosis) <- "Excess Kurtosis"
kurtosis.pvalue <- kurttest$p.value

# Jb test
robustjb_R <- DescTools::JarqueBeraTest(coredata(stdret.R)) #robust?
jb_R <- normtest::jb.norm.test(stdret.R)

jb_R <- paste0(round(jb_R$statistic,4),"***")
names(jb_R) <- "Jarque-Bera"

Statistics.S <- c(Statistics.S[1:5], Skewness, paste0("(",skewness.pvalue,"***)"), Excess_kurtosis, paste0("(",kurtosis.pvalue,"***)"),jb_R)

# for table 1

table1 <- data.frame(Statistics = names(Statistics.S), `Eurostoxx 50` = Statistics, `Standardized Residuals`= Statistics.S)

colnames(table1) <- gsub(pattern = '.', replacement=' ',colnames(table1), fixed=T)

# JB.Test <- rbind( c(jarque.bera.test(R)$statistic, jarque.bera.test(stdret.R)$statistic) ,
#                   c(jarque.bera.test(R)$p.value, jarque.bera.test(stdret.R)$p.value))
# rownames(JB.Test) <- c("Jarque - Bera", "JB p-values")
# colnames(JB.Test) <- colnames(table1)
# table1 <- rbind(table1, JB.Test)

table1 %>% kbl(caption = "Summary statistics of the returns",
      label = 'dsTable',
      booktabs = T,
      position = "h!",
      digits = 3 )%>%
  kable_classic(full_width = F)%>%
  footnote(sprintf('This table shows the descriptive statistics of the returns of %s over the period %s to %s. Including minimum, median, arithmetic average, maximum, standard deviation, skewness and excess kurtosis.',gsub(pattern = '.', replacement=' ',colnames(R)),gsub(" UTC", "",min(index(R))),gsub(" UTC", "",max(index(R)))),
           footnote_as_chunk = T, general_title = "Note: ")
```

#### Descriptive figures

```{r plot1, echo=F, fig.cap='Eurostoxx 50 prices and returns'}
#par(mfrow = c(1,1))
plot(Estoxx, major.ticks = "years", grid.ticks.on = "years", col = "blue",
     main = "EuroStoxx 50 Price Index")
plot(R, major.ticks = "years", grid.ticks.on = "years", col = "blue",
     main = "EuroStoxx 50 Price Log Returns")
```

```{r plot2, echo=F, fig.cap='Eurostoxx 50 rolling volatility (22 days, calculated over 252 days)'}
#Plotting volatility

chart.RollingPerformance(R = R, width = 22,
     FUN = "sd.annualized", scale = 252, main = "One month rolling volatility")
```


```{r plot3, echo=F}
#Histogram to eyeball normality

# h <- hist(R, breaks = 75, density = 10,
#           col = "lightgray", xlab = "Accuracy", main = "Barplot") 
# xfit <- seq(min(R), max(R), length = 40) 
# yfit <- dnorm(xfit, mean = mean(R), sd = sd(R)) 
# yfit <- yfit * diff(h$mids[1:2]) * length(R) 
# lines(xfit, yfit, col = "black", lwd = 1)

chart.Histogram(R = R, methods = "add.normal", breaks = 100, main = "Returns Histogram Vs. Normal")
```

As can be seen

```{r acfplots, echo=F}

# Compute the mean daily return
m <- mean(R)

# Define the series of prediction errors
e <- R - m

# Plot the absolute value of the prediction errors
par(mfrow = c(2,1),mar = c(1, 2, 2, 1))
plot(abs(e), main = "Absolute Prediction Error")

# Plot the acf of the absolute prediction errors
acf(abs(e), main = "", xaxt = "n")

# acf(coredata(R), lag = 22, main = "EuroStoxx 50 Price Log Returns")
# acf(coredata(abs(R)), lag = 22, main = "EuroStoxx 50 Absolute Price Log Returns")
```

\newpage

### Methodology

As already mentioned in ..., GARCH models sGARCH, eGARCH, iGARCH, gjrGARCH, nGARCH, tGARCH and tsGARCH will be estimated. Additionally the distributions will be examined as well, including the normal, student-t distribution, skewed student-t distribution, generalised error distribution, skewed generalised error distribution and the skewed generalised Theodossiou distribution.

They will be estimated using maximum likelihood. As already mentioned, fortunately, Alexios @alexios2020 has made it easy for us to implement this methodology in R (version 3.6.1) with the package "rugarch" version 1.4-4 (R univariate garch), which gives us a bit more time to focus on the results and the interpretation.

Maximum likelihood estimation is a method to find the distribution parameters that best fit the observed data, through maximization of the likelihood function, or the computationally more efficient log-likelihood function (by taking the natural logarithm). It is assumed that the return data is i.i.d. and that there is some underlying parametrized density function $f$ with one or more parameters $\theta$ that generates the data (\@ref(eq:eq1)). These functions are based on the joint probability distribution of the observed data (equation \@ref(eq:eq2)). Subsequently, the (log)likelihood function is maximized using an optimization algorithm (equation \@ref(eq:eq3)).

```{=tex}
\begin{align} 
     y_1,y_2,...,y_N \sim i.i.d
    \\
    y_i \sim f(y|\theta)
 (\#eq:eq1)
\end{align}
```
```{=tex}
\begin{align} 

 L(\theta) = \prod^{N}_{i=1}f(y_i|\theta)\\
\log(L(\theta)) = \sum^{N}_{i=1} \log f(y_i |\theta)

 (\#eq:eq2)
\end{align}
```
```{=tex}
\begin{align} 

\theta^{*} = arg \max_{\theta} \bigg[ {(L)} \bigg]
\\theta^{*} = arg \max_{\theta} \bigg[ \log{(L)} \bigg]

 (\#eq:eq3)
\end{align}
```
```{r MLE exampe}
# n_samples <- 25; true_rate <- 1; set.seed(1)
# exp_samples <- rexp(n = n_samples,
#                     rate = true_rate)
# 
# sample_data <- exp_samples
# rate_fit_R <- fitdistrplus::fitdist(data = sample_data, 
#                       distr = 'exp', 
#                       method = 'mle')
# 
# rate_fit_R$estimate
```

```{r MLE normdist}
# Normal
normfit_R <- fitdistrplus::fitdist(data = as.vector(R), 
                      distr = "norm",
                      method = "mle")
normfit_R$estimate #estimated parameters
normfit_R$sd #estimated standard errors
normfit_R$loglik #max log-likelihood 
#? # LR statistic

# T-dist
tstart <- list(df=1, ncp = 1)

tfit_R <- fitdistrplus::fitdist(data = as.vector(R), 
                      distr = "t",
                      method = "mle", start = tstart)
tfit_R$estimate #estimated parameters
tfit_R$sd #estimated standard errors
tfit_R$loglik #max log-likelihood 
#? # LR statistic

# GED distribution 
gedstart <- list(mean=1, sd=1, nu=1)
gedfit_R <- fitdistrplus::fitdist(data = as.vector(R), 
                      distr = "ged",
                      method = "mle", start = gedstart)
gedfit_R$estimate
gedfit_R$sd
gedfit_R$loglik
#? # LR statistic



# ST distribution
## Stephane's skewed student t is different from the rugarch package, so I would skip it? maybe it is just because of the different estimates
## skewtstart <- list(xi=0, omega=2, alpha=2, nu=8)
## skewtfit_R <- fitdistrplus::fitdist(data = as.vector(R),
##                       distr = "st",
##                       method = "mle", skewtstart)
## skewtfit_R$estimate
## skewtfit_R$sd
## skewtfit_R$loglik

STstart <- list(mean=0,sd=2, nu = 8, xi=2)
STfit_R <- fitdistrplus::fitdist(data = as.vector(coredata(R)), distr = "sstd", method = "mle", STstart)
STfit_R$estimate
STfit_R$sd
STfit_R$loglik
## ? # LR statistic

# ST <- rugarch::fitdist("sstd", x = R)
# ST$pars # parameters are very similar to the ones we got with fitdistrplus, which is good

#SGT distribution
X.f = X ~ coredata(R)
start = list(mu=0,sigma=2, lambda = 0, p=2, q=8)#list(mu=0,sigma=2, lambda = 0.5, p=2, q=12)
result = sgt.mle(X.f = X.f, start = start)
summary(result)

# SGTstart <- list(mu=0,sigma=2, lambda = 0.5, p=2, q=8)
# SGTfit_R <- fitdistrplus::fitdist(data = as.vector(coredata(R)), distr = "sgt", method = "mle", SGTstart)
# summary(SGTfit_R)
## ? # LR statistic

```

#### Control Tests

##### Unconditional coverage test of @kupiec1995

A number of tests are computed to see if the value-at-risk estimations capture the actual losses well. A first one is the unconditional coverage test by @kupiec1995. The unconditional coverage or proportion of failures method tests if the actual value-at-risk exceedances are consistent with the expected exceedances (a chosen percentile, e.g. 1% percentile) of the VaR model. Following @kupiec1995 and @ghalanos2020, the number of exceedence follow a binomial distribution (with thus probability equal to the significance level or expected proportion) under the null hypothesis of a correct VaR model. The test is conducted as a likelihood ratio test with statistic like in equation \@ref(eq:uccov), with $p$ the probability of an exceedence for a confidence level, $N$ the sample size and $X$ the number of exceedence. The null hypothesis states that the test statistic $L R^{u c}$ is $\chi^2$-distributed with one degree of freedom or that the probability of failure $\hat p$ is equal to the chosen percentile $\alpha$.

```{=tex}
\begin{aligned}
L R^{u c}=-2 \ln \left(\frac{(1-p)^{N-X} p^{X}}{\left(1-\frac{X}{N}\right)^{N-X}\left(\frac{X}{N}\right)^{X}}\right)
(\#eq:uccov)
\end{aligned}
```
##### Conditional coverage test of @christoffersen2001

@christoffersen2001 proposed the conditional coverage test. It is tests for unconditional covrage and serial independence. The serial independence is important while the $L R^{u c}$ can give a false picture while at any point in time it classifies inaccurate VaR estimates as "acceptably accurate" [@bali2007]. For a certain VaR estimate an indicator variable, $I_t(\alpha)$, is computed as equation \@ref(eq:ccov).

```{=tex}
\begin{aligned}
I_{t}(\alpha)=\left\{\begin{array}{ll}
1 & \text { if exceedence occurs } \\
0 & \text { if no exceedence occurs }
\end{array} .\right.
(\#eq:ccov)
\end{aligned}
```
It involves a likelihood ratio test's null hypothesis is that the statistic is $\chi^2$-distributed with two degrees of freedom or that the probability of violation $\hat p$ (unconditional coverage) as well as the conditional coverage (independence) is equal to the chosen percentile $\alpha$.

##### Dynamic quantile test

@engle2004 with the aim to provide completeness to the conditional coverage test of @christoffersen2001 developed the Dynamic quantile test. It consists in testing some restriction in a

\clearpage
