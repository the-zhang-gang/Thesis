---
#########################################
# options for knitting a single chapter #
#########################################
output:
  bookdown::pdf_document2:
    template: templates/brief_template.tex
    citation_package: biblatex
  bookdown::html_document2: default
  bookdown::word_document2: default
documentclass: book
bibliography: references.bib
---

# Introduction {.unnumbered}

```{=tex}
\adjustmtc
\markboth{Introduction}{}
```
A general assumption in finance is that stock returns are normally distributed (...). However, various authors have shown that this assumption does not hold in practice: stock returns are not normally distributed (...). For example, @theodossiou2000 mentions that "empirical distributions of log-returns of several financial assets exhibit strong higher-order moment dependencies which exist mainly in daily and weekly log-returns and prevent monthly, bimonthly and quarterly log-returns from obeying the normality law implied by the central limit theorem. As a consequence, price changes do not follow the geometric Brownian motion." So in reality, stock returns exhibit fat-tails and peakedness (...), these are some of the so-called stylized facts of returns. \

Additionally, a point of interest is the predictability of stock prices. @fama1965 explains that the question in academic and business circles is: "To what extent can the past history of a common stock's price be used to make meaningful predictions concerning the future price of the stock?". There are two viewpoints towards the predictability of stock prices. Firstly, some argue that stock prices are unpredictable or very difficult to predict by their past returns (i.e. have very little serial correlation) because they simply follow a Random Walk process (...). On the other hand, Lo & MacKinlay mention that "financial markets *are* predictable to some extent but far from being a symptom of inefficiency or irrationality, predictability is the oil that lubricates the gears of capitalism". Furthermore, there is also no real robust evidence for the predictability of returns themselves, let alone be out-of-sample [@welch2008]. This makes it difficult for corporations to manage market risk, i.e. the variability of stock prices.  \

Risk, in general, can be defined as the volatility of unexpected outcomes [@jorion2007]. The measure Value at Risk (VaR), developed in response to the financial disaster events of the early 1990s, has been very important in the financial world. Corporations have to manage their risks and thereby include a future risk measurement. The tool of VaR has now become a standard measure of risk for many financial institutions going from banks, that use VaR to calculate the adequacy of their capital structure, to other financial services companies to assess the exposure of their positions and portfolios. The 5% VaR can be informally defined as the maximum loss of a portfolio, during a time horizon, excluding all the negative events with a combined probability lower than 5% while the Conditional Value at Risk (CVaR) can be informally defined as the average of the events that are lower than the VaR. @bali2008 explains that many implementations of the CVaR have the assumption that asset and portfolio's returns are normally distributed but that it is an inconsistency with the evidence empirically available which outlines a more skewed distribution with fatter tails than the normal. This lead to the conclusion that the assumption of normality, which simplifies the computation of VaR, can bring to incorrect numbers, underestimating the probability of extreme events happening. \

This paper has the aim to replicate and update the research made by @bali2008 on US indexes, analyzing the dynamics proposed with a European outlook. The main contribution of the research is to provide the industry with a new approach to calculating VaR with a flexible tool for modeling the empirical distribution of returns with higher accuracy and characterization of the tails. \

The paper is organized as follows. Chapter \@ref(lit-rev) discusses at first the alternative distribution than the normal that we are going to evaluate during the analysis (Student's t-distribution, Generalized Error Distribution, Skewed t-distribution, Skewed Generalized Error Distribution, Skewed Generalized t-distribution), then the discrete time GARCH models used (GARCH, IGARCH, EGARCH, GJRGARCH, NAGARCH, TGARCH, TSGARCH or AVGARCH and EWMA) are presented as extensions of the @engle1982 's ARCH model. Chapter \@ref(dat-and-meth) describes the dataset used and the methodology followed in modeling the volatility with the GARCH model by @bollerslev1986 and with its refinements using Maximum likelihood estimation to find the distribution parameters. Then a description is given of how are performed the control tests (un- and conditional coverage test, dynamic quantile test) used in the paper to evaluate the performances of the different GARCH models and underlying distributions. In chapter \@ref(analysis), findings are presented and discussed, in chapter \@ref(Robustness) the findings of the performed tests are shown and interpreted and in chapter \@ref(Conclusion) the investigation and the results are summarized.
